#Calculate the difference
diff_cdf = Prob - normcdf
diff_cdf
prob
Prob
normcdf
#Calculate the Normal CDFs
normcdf = rep(pnorm(tvalues), 3)
normcdf
#Calculate the Normal CDFs
normcdf = matrix(rep(pnorm(tvalues), 3), ncol=5)
normcdf
#Calculate the Normal CDFs
normcdf = matrix(rep(pnorm(tvalues), 3), ncol=5, byrow=FALSE)
normcdf
#Calculate the Normal CDFs
normcdf = matrix(rep(pnorm(tvalues), 3), ncol=5, byrow=TRUE)
normcdf
#Calculate the difference
diff_cdf = Prob - normcdf
#Calculate the difference
diff_cdf = abs(Prob - normcdf)
diff_cdf
#Calculate the difference
diff_cdf = abs(Prob - normcdf)
diff_cdf
Prob
normcdf
diff_cdf
#The supremum of differences for different t
sup_diffs = c(
max(diff_cdf[1,]),
max(diff_cdf[2,]),
max(diff_cdf[3,])
)
sup_diffs
#Third absolute moment
b3 = mean(abs(sample)^3)
b^3
stribution with mean 0 and variance 1
sample = rexp(n=10000000, rate = 1)-1
#Summary stats
mean(sample)
var(sample)
#Third absolute moment
b3 = mean(abs(sample)^3)
b3
b3 = 12/exp(1) - 2
b3
BEupper = 0.4748*b3*sqrt(nsizes)
BEupper
nsizes
BEupper = 0.4748*b3/sqrt(nsizes)
BEupper
#Part B
#Number of values of Zi per sample
nsizes = c(100, 1000, 10000)
#Number of samples
nsamples = 1000
#Different thresholds for the empirical CDF
tvalues = c(0, 0.5, 1, 1.5, 2)
Prob = matrix(NA, nrow=length(nsizes), ncol = length(tvalues))
#Calculate probabilities
for (n in 1:length(nsizes)){
#generate random sample - "nsamples" samples for Zi of length n
Z = rexp(nsizes[n]*nsamples, rate = 1)-1
Z=matrix(Z, ncol=1000) #Reshape the sample
#get values of sum(Zi)/sqrt(n)
Zvals = (nsizes[n]^(-.5))*(colSums(Z))
#Expand size of tvalues
for (t in 1:length(tvalues)){
#Calculate the probability -- the number of values below the t threshold
Prob[n,t] = sum(Zvals <= tvalues[t])/length(Zvals)
}
}
#Calculate the Normal CDFs
normcdf = matrix(rep(pnorm(tvalues), 3), ncol=5, byrow=TRUE)
#Calculate the difference
diff_cdf = abs(Prob - normcdf)
Prob
normcdf
diff_cdf
#The supremum of differences for t across all n
sup_diffs = c(
max(diff_cdf[1,]),
max(diff_cdf[2,]),
max(diff_cdf[3,])
)
b3 = 12/exp(1) - 2
BEupper = 0.4748*b3/sqrt(nsizes)
BEupper
#Comparing the difference in supremum of differences and the upper bound given by Berry-Esseen
sup_diffs
BEupper
#Calculate the Normal CDFs
normcdf = matrix(rep(pnorm(tvalues), 3), ncol=5, byrow=TRUE)
#Calculate the difference
diff_cdf = abs(Prob - normcdf)
Prob
normcdf
diff_cdf
#The supremum of differences for t across all n
sup_diffs = c(
max(diff_cdf[1,]),
max(diff_cdf[2,]),
max(diff_cdf[3,])
)
#Calculate probabilities
for (n in 1:length(nsizes)){
#generate random sample - "nsamples" samples for Zi of length n
Z = rexp(nsizes[n]*nsamples, rate = 1)-1
Z=matrix(Z, ncol=1000) #Reshape the sample
#get values of sum(Zi)/sqrt(n)
Zvals = (nsizes[n]^(-.5))*(colSums(Z))
#Expand size of tvalues
for (t in 1:length(tvalues)){
#Calculate the probability -- the number of values below the t threshold
Prob[n,t] = sum(Zvals <= tvalues[t])/length(Zvals)
}
}
Z
dim(Z)
#Part B
#Number of values of Zi per sample
nsizes = c(100, 1000, 10000)
#Number of samples
nsamples = 10000
#Different thresholds for the empirical CDF
tvalues = c(0, 0.5, 1, 1.5, 2)
Prob = matrix(NA, nrow=length(nsizes), ncol = length(tvalues))
#Calculate probabilities
for (n in 1:length(nsizes)){
#generate random sample - "nsamples" samples for Zi of length n
Z = rexp(nsizes[n]*nsamples, rate = 1)-1
Z=matrix(Z, ncol=1000) #Reshape the sample
#get values of sum(Zi)/sqrt(n)
Zvals = (nsizes[n]^(-.5))*(colSums(Z))
#Expand size of tvalues
for (t in 1:length(tvalues)){
#Calculate the probability -- the number of values below the t threshold
Prob[n,t] = sum(Zvals <= tvalues[t])/length(Zvals)
}
}
#Calculate the Normal CDFs
normcdf = matrix(rep(pnorm(tvalues), 3), ncol=5, byrow=TRUE)
#Calculate the difference
diff_cdf = abs(Prob - normcdf)
Prob
n
nsizes[n]
#generate random sample - "nsamples" samples for Zi of length n
Z = rexp(nsizes[n]*nsamples, rate = 1)-1
Z=matrix(Z, ncol=1000) #Reshape the sample
#get values of sum(Zi)/sqrt(n)
Zvals = (nsizes[n]^(-.5))*(colSums(Z))
dim(Zvals)
length(Zvals)
#Part B
#Number of values of Zi per sample
nsizes = c(100, 1000, 10000)
#Number of samples
nsamples = 10000
#Different thresholds for the empirical CDF
tvalues = c(0, 0.5, 1, 1.5, 2)
Prob = matrix(NA, nrow=length(nsizes), ncol = length(tvalues))
#Calculate probabilities
for (n in 1:length(nsizes)){
#generate random sample - "nsamples" samples for Zi of length n
Z = rexp(nsizes[n]*nsamples, rate = 1)-1
Z=matrix(Z, ncol=nsamples) #Reshape the sample
#get values of sum(Zi)/sqrt(n)
Zvals = (nsizes[n]^(-.5))*(colSums(Z))
#Expand size of tvalues
for (t in 1:length(tvalues)){
#Calculate the probability -- the number of values below the t threshold
Prob[n,t] = sum(Zvals <= tvalues[t])/length(Zvals)
}
}
#Calculate the Normal CDFs
normcdf = matrix(rep(pnorm(tvalues), 3), ncol=5, byrow=TRUE)
#Calculate the difference
diff_cdf = abs(Prob - normcdf)
Prob
normcdf
diff_cdf
#The supremum of differences for t across all n
sup_diffs = c(
max(diff_cdf[1,]),
max(diff_cdf[2,]),
max(diff_cdf[3,])
)
b3 = 12/exp(1) - 2
BEupper = 0.4748*b3/sqrt(nsizes)
#Comparing the difference in supremum of differences and the upper bound given by Berry-Esseen
sup_diffs
BEupper
Prob
diff_cdf
#The supremum of differences for t across all n
sup_diffs = c(
max(diff_cdf[1,]),
max(diff_cdf[2,]),
max(diff_cdf[3,])
)
#Comparing the difference in supremum of differences and the upper bound given by Berry-Esseen
sup_diffs
#Exponential distribution
library(stats)
#install.packages("moments")
library(moments)
wald_noncentral = function(n, thetastar, theta0){
n*(thetastar - theta0)^2/theta0^2
}
alpha=0.05
nvals = c(25,50,100)
thetavals = c(1.0,1.2,1.4,1.6,1.8,2.0)
theta0=1
probs_rejection = matrix(data=NA,nrow=length(thetavals),ncol=length(nvals))
for(i in 1:length(nvals)){
for (j in 1:length(thetavals)){
#Calculate probability of rejection
MU = wald_noncentral(nvals[i], thetavals[j], theta0)
#Get chisquared quantile
qtile = qchisq(1-alpha, df=1)
#Probability that Wald > quantile_{1-0.05}
probs_rejection[j, i] = pchisq(qtile, df=1, ncp = MU, lower.tail = FALSE, log.p = FALSE)
}
}
probs_rejection
#Wald finite sample stat
wald_finite = function(X, n, thetastar){
Xbar = mean(X)
#Vn = -1/(Xbar^2) + 2/(Xbar^3)*X^bar
Vn = 1/(Xbar)^2
#n*(Xbar - theta0)^2/Vn\
n*(Xbar - thetastar)^2/Vn
}
probs_rejection
#Wald finite sample stat
wald_finite = function(X, n, thetastar){
Xbar = mean(X)
#Vn = -1/(Xbar^2) + 2/(Xbar^3)*X^bar
Vn = 1/(Xbar)^2
#n*(Xbar - theta0)^2/Vn\
n*(Xbar - thetastar)^2/Vn
}
#LR finite sample test stat
LR_finite = function(X, n, thetastar){
Xbar = mean(X)
2*n*(Xbar/thetastar - 1 - log(Xbar/thetastar))
}
#LM finite sample test stat
LM_finite = function(X, n, thetastar){
Xbar = mean(X)
#term1 = (n/thetastar^2)*(Xbar - thetastar)
term1 = (-n/thetastar + (n/thetastar^2)*Xbar)/sqrt(n)
term2 = ((1/thetastar^2) - (2/thetastar^3)*Xbar)
#term1 = (-n/thetastar + (n/thetastar^2)*Xbar)^2/n
#term2 = mean((-n/thetastar + (n/thetastar^2)*X)^2)
term1/term2
}
#Generate random sample
probs_reject_mc_wald = matrix(data=NA,nrow=length(thetavals),ncol=length(nvals))
probs_reject_mc_LR = matrix(data=NA,nrow=length(thetavals),ncol=length(nvals))
probs_reject_mc_LM = matrix(data=NA,nrow=length(thetavals),ncol=length(nvals))
for(i in 1:length(nvals)){
for (j in 1:length(thetavals)){
#Calculate probability of rejection
X0 = rexp(1000*nvals[i], rate = theta0)
X=matrix(X0, nrow=nvals[i])
critical_value = qchisq(p=1-alpha, df=1)
#### Wald Test ####
Wald = apply(X, MARGIN = 2, FUN = function(x) wald_finite(x, nvals[i], thetavals[j]))
#Get the critical value
#MU = wald_noncentral(nvals[i], thetavals[i], theta0)
#critical_value = qchisq(p=1-alpha, ncp=MU, df=1)
#Get the probability of
probs_reject_mc_wald[j,i] = sum(Wald>critical_value)/length(Wald)
#### LR Test ####
LR = apply(X, MARGIN=2, FUN = function(x) LR_finite(x, nvals[i], thetavals[j]))
probs_reject_mc_LR[j,i] = sum(LR>critical_value)/length(LR)
#### LM Test ####
LM = apply(X, MARGIN=2, FUN = function(x) LM_finite(x, nvals[i], thetavals[j]))
probs_reject_mc_LM[j,i] = sum(LM>critical_value)/length(LM)
}
}
probs_reject_mc_wald
probs_rejection
probs_reject_mc_LR
probs_reject_mc_LM
#LM finite sample test stat
LM_finite = function(X, n, thetastar){
Xbar = mean(X)
#term1 = (n/thetastar^2)*(Xbar - thetastar)/n
#term1 = (-n/thetastar + (n/thetastar^2)*Xbar)/sqrt(n)
term1 = (-n/thetastar + (n/thetastar^2)*Xbar)^2/n
term2 = ((1/thetastar^2) - (2/thetastar^3)*Xbar)
#term2 = mean((-n/thetastar + (n/thetastar^2)*X)^2)
term1/term2
}
#Generate random sample
probs_reject_mc_wald = matrix(data=NA,nrow=length(thetavals),ncol=length(nvals))
probs_reject_mc_LR = matrix(data=NA,nrow=length(thetavals),ncol=length(nvals))
probs_reject_mc_LM = matrix(data=NA,nrow=length(thetavals),ncol=length(nvals))
for(i in 1:length(nvals)){
for (j in 1:length(thetavals)){
#Calculate probability of rejection
X0 = rexp(1000*nvals[i], rate = theta0)
X=matrix(X0, nrow=nvals[i])
critical_value = qchisq(p=1-alpha, df=1)
#### Wald Test ####
Wald = apply(X, MARGIN = 2, FUN = function(x) wald_finite(x, nvals[i], thetavals[j]))
#Get the critical value
#MU = wald_noncentral(nvals[i], thetavals[i], theta0)
#critical_value = qchisq(p=1-alpha, ncp=MU, df=1)
#Get the probability of
probs_reject_mc_wald[j,i] = sum(Wald>critical_value)/length(Wald)
#### LR Test ####
LR = apply(X, MARGIN=2, FUN = function(x) LR_finite(x, nvals[i], thetavals[j]))
probs_reject_mc_LR[j,i] = sum(LR>critical_value)/length(LR)
#### LM Test ####
LM = apply(X, MARGIN=2, FUN = function(x) LM_finite(x, nvals[i], thetavals[j]))
probs_reject_mc_LM[j,i] = sum(LM>critical_value)/length(LM)
}
}
probs_reject_mc_wald
probs_rejection
probs_reject_mc_LR
probs_reject_mc_LM
#GMM
library(tidyverse)
?read_excle
readxl("AJR2001.xlsx")
read_excel("AJR2001.xlsx")
library(readxl)
read_excel("AJR2001.xlsx")
getwd()
getwd
getwd()
x = -100:100
x
x3 = x^3
model = lm(x3~x, data = data.frame(x3, x))
model
x = -10:10
x3 = x^3
model = lm(x3~x, data = data.frame(x3, x))
model
x=randn(100)
x
rnorm(100)
x = rnorm(100)
x3=x^3
model = lm(x3~x, data = data.frame(x3, x))
model
x = rnorm(100,10)
model
model = lm(x3~x, data = data.frame(x3, x))
model
x = rnorm(100)
x = rnorm(10000)
x3=x^3
model = lm(x3~x, data = data.frame(x3, x))
model
x = rnorm(100, sd=4)
x3=x^3
model = lm(x3~x, data = data.frame(x3, x))
model
rnorm(100)
x = rnorm(100, sd=sqrt(2))
model = lm(x3~x, data = data.frame(x3, x))
x3=x^3
model = lm(x3~x, data = data.frame(x3, x))
model
x = rnorm(10000, sd=sqrt(2))
x3=x^3
model = lm(x3~x, data = data.frame(x3, x))
model
#Kernel density estimation
library(tidyverse)
library(readxl)
library(haven)
library(cmna)
#From ALFRED archives (to use the same vintage of data used by DiNardo and Tobias (2001))
#https://alfred.stlouisfed.org/series?seid=CPIAUCSL
Vintage2001CPI_1979 = 76.90
Vintage2001CPI_1989 = 126.40
Vintage2001CPI_2000 = 174.60
dollars79_2000 = Vintage2001CPI_2000/Vintage2001CPI_1979
dollars89_2000 = Vintage2001CPI_2000/Vintage2001CPI_1989
# Read in the MORG data
data79_raw <- read_dta("morg79.dta")
data89_raw <- read_dta("morg89.dta")
var(-3:3)
var(0:3)
var(0:3)*2
var(-10:10)*2
var(-10:10)
var(0:10)
var(0:11)
data <- read.delim("oe_series/oe_data_1_AllData", header = TRUE)
data <- read.delim("oe_series/oe_data_1_AllData.txt", header = TRUE)
getwd()
setwd("C:\Users\Russe\Desktop\IO-Problem-Sets\PS3_Russell\Code_Python")
setwd("C:/Users/Russe/Desktop/IO-Problem-Sets/PS3_Russell/Code_Python")
getwd()
install.packages("gnrprod")
#install.packages("gnrprod")
library(gnrprod)
View(gnrflex)
View(gauss_newton_reg)
#install.packages("gnrprod")
library(gnrprod)
View(gnrflex)
#View(gnrflex)
library(readxl)
library(dplyr)
# Load the Excel file
filename <- "../PS3_data_changedtoxlsx.xlsx"
df0 <- read_excel(filename)
# Select specific columns and rename them
df <- df0 %>%
select(year, firm_id, X03, X04, X05, X16, X40, X43, X44, X45, X49) %>%
rename(t = year,
firm_id = firm_id,
y_gross = X03,
s01 = X04,
s02 = X05,
s13 = X16,
k = X40,
l = X43,
m = X44,
py = X45,
pm = X49)
# Drop rows where 'm' is 0 and filter for industry 1 only
df <- df %>%
filter(m != 0, s13 == 1)
# Create new variables
df <- df %>%
mutate(y = y_gross,
s = pm + m - py - y)
# Sort by 'firm_id' and 't' for lagged variables
df <- df %>%
arrange(firm_id, t) %>%
group_by(firm_id) %>%
mutate(kprev = lag(k),
lprev = lag(l),
mprev = lag(m)) %>%
ungroup()
# View the final dataframe
print(df)
# View the final dataframe
print(df)
gnrflex(output = "RGO", fixed = c("l", "k"),
flex = "m", share = "share", id = "firm_id",
time = "t", data = data,
control = list(degree = 2, maxit = 200))
gnrflex(output = "RGO", fixed = c("l", "k"),
flex = "m", share = "share", id = "firm_id",
time = "t", data = df,
control = list(degree = 2, maxit = 200))
gnrflex(output = "y", fixed = c("l", "k"),
flex = "m", share = "share", id = "firm_id",
time = "t", data = df,
control = list(degree = 2, maxit = 200))
# View the final dataframe
print(df)
gnrflex(output = "y", fixed = c("l", "k"),
flex = "m", share = "s", id = "firm_id",
time = "t", data = df,
control = list(degree = 2, maxit = 200))
OUT = gnrflex(output = "y", fixed = c("l", "k"),
flex = "m", share = "s", id = "firm_id",
time = "t", data = df,
control = list(degree = 2, maxit = 200))
View(OUT)
# View the final dataframe
print(df)
OUT = gnrflex(output = "y", fixed = c("l", "k"),
flex = "m", share = "s", id = "firm_id",
time = "t", data = df,
control = list(degree = 2, maxit = 200))
View(OUT)
plot(OUT$elas$residuals)
# Load the Excel file
filename <- "../PS3_data_changedtoxlsx.xlsx"
df0 <- read_excel(filename)
# Select specific columns and rename them
df <- df0 %>%
select(year, firm_id, X03, X04, X05, X16, X40, X43, X44, X45, X49) %>%
rename(t = year,
firm_id = firm_id,
y_gross = X03,
s01 = X04,
s02 = X05,
s13 = X16,
k = X40,
l = X43,
m = X44,
py = X45,
pm = X49)
View(gnrflex)
ctrl <- gnrflex.control()
if (!missing(control)) {
control <- as.list(control)
ctrl[names(control)] <- control
}
output <- get_matrix(output, data)
output = df$y_gross
View(gauss_newton_reg)
View(grnprod::gauss_newton_reg)
View(gnrprod::gauss_newton_reg)
